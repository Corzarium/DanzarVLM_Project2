# services/vision_aware_conversation_service.py
import asyncio
import time
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from collections import deque
import threading

# Import CLIP vision enhancer
try:
    from services.clip_vision_enhancer import CLIPVisionEnhancer
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    CLIPVisionEnhancer = None

# Import enhanced conversation memory
try:
    from services.enhanced_conversation_memory import EnhancedConversationMemory
    ENHANCED_MEMORY_AVAILABLE = True
except ImportError:
    ENHANCED_MEMORY_AVAILABLE = False
    EnhancedConversationMemory = None

@dataclass
class VisualContext:
    """Represents current visual context from vision models"""
    timestamp: float
    detected_objects: List[Dict[str, Any]]  # YOLO detections
    ocr_text: List[str]  # OCR results
    ui_elements: List[Dict[str, Any]]  # Template matches
    scene_summary: str  # High-level scene description
    confidence: float  # Overall confidence in visual understanding
    game_context: Dict[str, Any]  # Game-specific context
    clip_insights: Optional[Dict[str, Any]] = None  # CLIP semantic understanding

@dataclass
class ConversationTurn:
    """Represents a conversation turn with visual context"""
    user_input: str
    visual_context: Optional[VisualContext]
    timestamp: float
    response: Optional[str] = None
    visual_references: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.visual_references is None:
            self.visual_references = []

class VisionAwareConversationService:
    """
    Service that enables natural conversation with vision awareness.
    The VLM can "see" and naturally reference visual elements in conversation.
    Enhanced with CLIP for semantic visual understanding and proper memory management.
    """
    
    def __init__(self, app_context):
        self.app_context = app_context
        self.logger = app_context.logger
        self.config = app_context.global_settings
        
        # Enhanced Memory System - STM in RAM, LTM in RAG
        self.conversation_memory = None
        if ENHANCED_MEMORY_AVAILABLE and EnhancedConversationMemory:
            try:
                self.conversation_memory = EnhancedConversationMemory(app_context)
                if self.conversation_memory:
                    self.conversation_memory.start_background_tasks()
                    self.logger.info("[VisionAwareConversation] Enhanced conversation memory initialized")
            except Exception as e:
                self.logger.error(f"[VisionAwareConversation] Failed to initialize enhanced memory: {e}")
                self.conversation_memory = None
        
        # Visual context management - SLOWED DOWN to prevent system overload
        self.current_visual_context: Optional[VisualContext] = None
        self.visual_context_history: deque = deque(maxlen=20)  # Reduced from 50
        self.last_visual_update: float = 0
        self.visual_update_interval: float = 10.0  # Increased to 10.0 seconds to match 1 FPS vision processing
        
        # Legacy conversation management (fallback if enhanced memory not available)
        self.conversation_history: deque = deque(maxlen=50)  # Reduced from 100 for better focus
        self.conversation_summary: str = ""  # Track conversation summary
        self.last_conversation_time: float = 0
        self.conversation_context_window: float = 300.0  # 5 minutes of context
        
        # Vision integration settings
        self.vision_integration_enabled: bool = True
        self.visual_context_threshold: float = 0.6
        self.max_visual_elements: int = 3  # Reduced from 5
        
        # CLIP integration
        self.clip_enhancer = None
        self.clip_enabled = CLIP_AVAILABLE
        
        # Threading
        self.visual_context_lock = threading.Lock()
        self.conversation_lock = threading.Lock()
        
        # Initialize CLIP if available
        if self.clip_enabled and CLIPVisionEnhancer:
            try:
                self.clip_enhancer = CLIPVisionEnhancer(self.app_context)
                self.logger.info("[VisionAwareConversation] CLIP vision enhancer initialized")
            except Exception as e:
                self.logger.error(f"[VisionAwareConversation] Failed to initialize CLIP: {e}")
                self.clip_enhancer = None
                self.clip_enabled = False
        
        self.logger.info(f"[VisionAwareConversation] Initialized with CLIP: {self.clip_enabled}")
        self.logger.info(f"[VisionAwareConversation] Enhanced memory: {self.conversation_memory is not None}")
        self.logger.info(f"[VisionAwareConversation] Visual update interval: {self.visual_update_interval}s")
    
    async def initialize(self) -> bool:
        """Initialize the vision-aware conversation service."""
        try:
            self.logger.info("[VisionAwareConversation] Initializing...")
            
            # Set up visual context monitoring with reduced frequency
            await self._start_visual_context_monitor()
            
            self.logger.info("[VisionAwareConversation] Initialization complete")
            return True
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Initialization failed: {e}", exc_info=True)
            return False
    
    async def _start_visual_context_monitor(self):
        """Start monitoring for visual context updates with reduced frequency."""
        async def monitor_loop():
            while not self.app_context.shutdown_event.is_set():
                try:
                    if self.vision_integration_enabled:
                        await self._update_visual_context()
                    
                    # Increased sleep time to reduce processing load
                    await asyncio.sleep(2.0)  # Increased from 1.0 to 2.0 seconds
                    
                except Exception as e:
                    self.logger.error(f"[VisionAwareConversation] Visual context monitor error: {e}")
                    await asyncio.sleep(10.0)  # Increased from 5.0 to 10.0 seconds
        
        asyncio.create_task(monitor_loop())
    
    async def _update_visual_context(self):
        """Update current visual context from vision models with CLIP enhancement."""
        try:
            # Get latest vision data from app context
            vision_data = getattr(self.app_context, 'latest_vision_data', None)
            if not vision_data:
                return
            
            current_time = time.time()
            
            # Only update if enough time has passed (increased interval)
            if current_time - self.last_visual_update < self.visual_update_interval:
                return
            
            with self.visual_context_lock:
                # Extract visual information
                detected_objects = vision_data.get('yolo_detections', [])
                ocr_text = vision_data.get('ocr_results', [])
                ui_elements = vision_data.get('template_matches', [])
                current_frame = vision_data.get('current_frame')
                
                # Get game context
                game_context = self._get_game_context()
                game_type = game_context.get('game_name', 'generic_game').lower()
                
                # Enhanced CLIP integration - more frequent processing
                clip_insights = None
                if self.clip_enhancer and current_frame is not None:
                    try:
                        # Process CLIP more frequently for better visual understanding
                        if current_time % 5 < 1:  # Every 5 seconds instead of 10
                            clip_insights = self.clip_enhancer.enhance_visual_context(
                                current_frame, detected_objects, ocr_text, game_type
                            )
                            self.logger.info(f"[VisionAwareConversation] CLIP enhanced context with {len(clip_insights.get('clip_insights', []))} insights")
                    except Exception as e:
                        self.logger.error(f"[VisionAwareConversation] CLIP enhancement error: {e}")
                
                # Create enhanced scene summary with CLIP insights
                scene_summary = self._create_enhanced_scene_summary(
                    detected_objects, ocr_text, ui_elements, clip_insights
                )
                
                # Calculate overall confidence
                confidence = self._calculate_enhanced_visual_confidence(
                    detected_objects, ocr_text, ui_elements, clip_insights
                )
                
                # Create new visual context
                new_context = VisualContext(
                    timestamp=current_time,
                    detected_objects=detected_objects,
                    ocr_text=ocr_text,
                    ui_elements=ui_elements,
                    scene_summary=scene_summary,
                    confidence=confidence,
                    game_context=game_context,
                    clip_insights=clip_insights
                )
                
                # Update current context
                if self.current_visual_context:
                    self.visual_context_history.append(self.current_visual_context)
                
                self.current_visual_context = new_context
                self.last_visual_update = current_time
                
                # Store in app context for other services to access
                if hasattr(self.app_context, 'current_visual_context'):
                    self.app_context.current_visual_context = new_context
                
                self.logger.info(f"[VisionAwareConversation] Updated visual context: {scene_summary[:100]}...")
                
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Visual context update error: {e}")
    
    def _create_enhanced_scene_summary(self, detected_objects: List, ocr_text: List, 
                                     ui_elements: List, clip_insights: Optional[Dict]) -> str:
        """Create an enhanced scene summary using CLIP insights."""
        summary_parts = []
        
        # Add CLIP insights if available
        if clip_insights and clip_insights.get('clip_insights'):
            clip_descriptions = clip_insights.get('visual_descriptions', [])
            if clip_descriptions:
                summary_parts.append(f"Visual analysis: {', '.join(clip_descriptions[:3])}")
        
        # Add detected objects
        if detected_objects:
            obj_summary = []
            for obj in detected_objects[:3]:  # Limit to top 3
                label = obj.get('label', 'unknown')
                conf = obj.get('confidence', 0)
                obj_summary.append(f"{label} ({conf:.2f})")
            if obj_summary:
                summary_parts.append(f"Detected: {', '.join(obj_summary)}")
        
        # Add OCR text
        if ocr_text:
            text_summary = []
            for text in ocr_text[:3]:  # Limit to top 3
                if len(text.strip()) > 2:  # Only meaningful text
                    text_summary.append(f'"{text.strip()}"')
            if text_summary:
                summary_parts.append(f"Text: {', '.join(text_summary)}")
        
        # Add UI elements
        if ui_elements:
            ui_summary = []
            for ui in ui_elements[:2]:  # Limit to top 2
                label = ui.get('label', 'ui_element')
                ui_summary.append(label)
            if ui_summary:
                summary_parts.append(f"UI: {', '.join(ui_summary)}")
        
        # Create final summary
        if summary_parts:
            return " | ".join(summary_parts)
        else:
            return "No significant visual elements detected"
    
    def _calculate_enhanced_visual_confidence(self, detected_objects: List, ocr_text: List, 
                                            ui_elements: List, clip_insights: Optional[Dict]) -> float:
        """Calculate enhanced confidence using CLIP insights."""
        base_confidence = self._calculate_visual_confidence(detected_objects, ocr_text, ui_elements)
        
        # Boost confidence if CLIP provides strong insights
        if clip_insights and clip_insights.get('clip_insights'):
            clip_confidence = max([insight['confidence'] for insight in clip_insights['clip_insights']], default=0.0)
            # Blend base confidence with CLIP confidence
            enhanced_confidence = (base_confidence * 0.6) + (clip_confidence * 0.4)
            return min(enhanced_confidence, 1.0)
        
        return base_confidence
    
    def _calculate_visual_confidence(self, detected_objects: List, ocr_text: List, ui_elements: List) -> float:
        """Calculate overall confidence in visual understanding."""
        if not detected_objects and not ocr_text and not ui_elements:
            return 0.0
        
        total_confidence = 0.0
        total_weight = 0.0
        
        # Weight object detections
        for obj in detected_objects:
            conf = obj.get('confidence', 0.5)
            total_confidence += conf * 0.4
            total_weight += 0.4
        
        # Weight OCR results
        for text in ocr_text:
            if len(text.strip()) > 3:
                total_confidence += 0.8 * 0.3
                total_weight += 0.3
                break
        
        # Weight UI elements
        for elem in ui_elements:
            conf = elem.get('confidence', 0.7)
            total_confidence += conf * 0.3
            total_weight += 0.3
            break
        
        return total_confidence / total_weight if total_weight > 0 else 0.0
    
    def _get_game_context(self) -> Dict[str, Any]:
        """Get current game context from app context."""
        context = {}
        
        # Get active game profile
        if hasattr(self.app_context, 'active_profile') and self.app_context.active_profile:
            context['game_name'] = self.app_context.active_profile.game_name
        
        # Get recent conversation context
        if self.conversation_history:
            recent_turns = list(self.conversation_history)[-3:]
            context['recent_conversation'] = [
                turn.user_input for turn in recent_turns
            ]
        
        return context
    
    async def process_conversation(self, user_input: str, include_visual_context: bool = True) -> str:
        """Process a conversation turn with visual awareness."""
        try:
            current_time = time.time()
            
            # Get current visual context if requested
            visual_context = None
            if include_visual_context and self.current_visual_context:
                visual_context = self.current_visual_context
            
            # Create conversation turn
            turn = ConversationTurn(
                user_input=user_input,
                visual_context=visual_context,
                timestamp=current_time
            )
            
            # Store in enhanced memory system
            if self.conversation_memory:
                # Add user input to STM
                self.conversation_memory.add_conversation_entry(
                    user_name="VirtualAudio",
                    content=user_input,
                    entry_type='user_input',
                    visual_context=visual_context.__dict__ if visual_context else None
                )
            
            # Generate response with visual awareness
            response = await self._generate_visual_aware_response(turn)
            
            # Store response in enhanced memory system
            if self.conversation_memory:
                self.conversation_memory.add_conversation_entry(
                    user_name="VirtualAudio",
                    content=response,
                    entry_type='bot_response',
                    visual_context=visual_context.__dict__ if visual_context else None
                )
            
            # Update conversation history (legacy fallback)
            with self.conversation_lock:
                self.conversation_history.append(turn)
                self.last_conversation_time = current_time
                self._update_conversation_summary(turn)
            
            self.logger.info(f"[VisionAwareConversation] Processed conversation turn: {len(user_input)} chars input, {len(response)} chars response")
            return response
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Conversation processing error: {e}")
            return "I'm having trouble processing that right now. Could you try again?"
    
    async def process_user_message(self, user_id: str, user_name: str, message: str, 
                                 game_context: Optional[str] = None) -> Optional[str]:
        """Process a user message with enhanced memory and visual context."""
        try:
            current_time = time.time()
            
            # Check for screenshot-related queries first
            if self._detect_screenshot_query(message):
                self.logger.info(f"[VisionAwareConversation] 📸 Detected screenshot query: '{message}'")
                return await self._handle_screenshot_query(user_id, user_name, message, game_context)
            
            # Get current visual context
            visual_context = None
            if self.current_visual_context:
                visual_context = self.current_visual_context
            
            # Store user message in enhanced memory
            if self.conversation_memory:
                self.conversation_memory.add_conversation_entry(
                    user_name=user_name,
                    content=message,
                    entry_type='user_input',
                    metadata={'user_id': user_id, 'game_context': game_context},
                    visual_context=visual_context.__dict__ if visual_context else None
                )
            
            # Get conversation context from enhanced memory
            conversation_context = None
            if self.conversation_memory:
                conversation_context = self.conversation_memory.get_conversation_context(
                    user_name=user_name,
                    include_ltm=True,
                    max_stm_entries=10,
                    max_ltm_results=3
                )
                self.logger.info(f"[VisionAwareConversation] Retrieved context for {user_name}: {len(conversation_context.get('stm_entries', []))} STM entries, {len(conversation_context.get('ltm_results', []))} LTM results")
            
            # Create conversation turn with enhanced context
            turn = ConversationTurn(
                user_input=message,
                visual_context=visual_context,
                timestamp=current_time
            )
            
            # Generate response with enhanced context
            response = await self._generate_enhanced_context_response(turn, conversation_context)
            
            # Store response in enhanced memory
            if self.conversation_memory:
                self.conversation_memory.add_conversation_entry(
                    user_name=user_name,
                    content=response,
                    entry_type='bot_response',
                    metadata={'user_id': user_id, 'game_context': game_context},
                    visual_context=visual_context.__dict__ if visual_context else None
                )
            
            self.logger.info(f"[VisionAwareConversation] Processed message for {user_name}: {len(message)} chars input, {len(response)} chars response")
            return response
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] User message processing error: {e}")
            return "I'm having trouble processing that right now. Could you try again?"
    
    def _detect_screenshot_query(self, message: str) -> bool:
        """Detect if the user is asking about what's happening in the game."""
        screenshot_keywords = [
            "what's happening", "what is happening", "whats happening",
            "what's going on", "what is going on", "whats going on",
            "what do you see", "what can you see", "what are you seeing",
            "show me", "take a screenshot", "capture", "what's on screen",
            "what's in the game", "what is in the game", "whats in the game",
            "describe the screen", "describe what you see", "tell me what you see",
            "what's on the screen", "what is on the screen", "whats on the screen",
            "current situation", "game state", "what's the situation",
            "what is the situation", "whats the situation",
            "can you see", "are you seeing", "do you see",
            "vision", "sight", "eyes", "looking", "watching"
        ]
        
        message_lower = message.lower()
        return any(keyword in message_lower for keyword in screenshot_keywords)
    
    async def _handle_screenshot_query(self, user_id: str, user_name: str, message: str, game_context: Optional[str] = None) -> str:
        """Handle screenshot-related queries by capturing and analyzing the current screen."""
        try:
            self.logger.info(f"[VisionAwareConversation] 📸 Handling screenshot query: '{message}'")
            
            # Get vision tools for function calling - FIX: Use vision_tools directly
            vision_tools = None
            if hasattr(self.app_context, 'vision_tools') and self.app_context.vision_tools:
                vision_tools = self.app_context.vision_tools.get_tools_for_llm()
                self.logger.info(f"[VisionAwareConversation] 📸 Retrieved {len(vision_tools)} vision tools")
            else:
                self.logger.warning("[VisionAwareConversation] ⚠️ No vision_tools available")
            
            # Create enhanced system prompt that forces tool usage
            system_prompt = self.app_context.active_profile.system_prompt_chat if self.app_context and hasattr(self.app_context, 'active_profile') else "You are DANZAR, a vision-capable gaming assistant with a witty personality."
            
            enhanced_system_prompt = f"""{system_prompt}

IMPORTANT: You have VISION CAPABILITIES and can see the user's screen through vision tools.
When asked about what's on screen, you MUST use the available vision tools to capture and analyze screenshots.

AVAILABLE VISION TOOLS:
- capture_screenshot: Captures the current screen
- analyze_screenshot: Analyzes a screenshot and provides detailed description
- get_vision_summary: Gets a summary of current visual context
- check_vision_capabilities: Checks if vision tools are working

INSTRUCTIONS:
1. When asked about the screen, first use capture_screenshot to get the current view
2. Then use analyze_screenshot to get a detailed description of what you see
3. Based on the analysis, provide a helpful, enthusiastic response in your DANZAR personality
4. Reference specific details from the screenshot analysis in your response

DO NOT say you cannot see the screen - you have vision tools available!"""

            # Create messages with proper system/user roles
            messages = [
                {"role": "system", "content": enhanced_system_prompt},
                {"role": "user", "content": f"User asks: {message}"}
            ]
            
            # Get model client directly from app context
            model_client = getattr(self.app_context, 'model_client', None)
            if not model_client:
                # Try alternative attribute names
                model_client = getattr(self.app_context, 'llm_client', None)
            if not model_client:
                # Try getting from services
                model_client = self.app_context.get_service('model_client')
            if not model_client:
                self.logger.error("[VisionAwareConversation] ❌ No model client available")
                return "I'm having trouble connecting to my language model right now."
            
            # First attempt: Try with tools if available
            if vision_tools:
                self.logger.info(f"[VisionAwareConversation] 🔧 Attempting tool-based response with {len(vision_tools)} tools")
                
                try:
                    # Call model with tools
                    response_data = await model_client.generate(
                        messages=messages,
                        tools=vision_tools,
                        temperature=0.7,
                        max_tokens=512
                    )
                    
                    # Check for tool calls in response
                    if response_data and 'tool_calls' in response_data:
                        self.logger.info(f"[VisionAwareConversation] 🛠️ Model requested {len(response_data['tool_calls'])} tool calls")
                        
                        tool_results = []
                        for tool_call in response_data['tool_calls']:
                            tool_name = tool_call.get('function', {}).get('name')
                            tool_args = tool_call.get('function', {}).get('arguments', '{}')
                            
                            if tool_name and hasattr(self.app_context.vision_tools, tool_name):
                                try:
                                    # Execute the tool
                                    if tool_name == 'capture_screenshot':
                                        result = await self.app_context.vision_tools.capture_screenshot()
                                    elif tool_name == 'analyze_screenshot':
                                        result = await self.app_context.vision_tools.analyze_screenshot()
                                    elif tool_name == 'get_vision_summary':
                                        result = await self.app_context.vision_tools.get_vision_summary()
                                    elif tool_name == 'check_vision_capabilities':
                                        result = await self.app_context.vision_tools.check_vision_capabilities()
                                    else:
                                        result = None
                                    
                                    if result and result.success:
                                        tool_results.append({
                                            "role": "tool",
                                            "tool_call_id": tool_call.get('id', ''),
                                            "content": result.data
                                        })
                                        self.logger.info(f"[VisionAwareConversation] ✅ Tool {tool_name} executed successfully")
                                    else:
                                        self.logger.warning(f"[VisionAwareConversation] ⚠️ Tool {tool_name} failed or returned no data")
                                        
                                except Exception as e:
                                    self.logger.error(f"[VisionAwareConversation] ❌ Error executing tool {tool_name}: {e}")
                            
                        # If we got tool results, call model again with results
                        if tool_results:
                            messages.extend(tool_results)
                            messages.append({"role": "user", "content": "Based on the tool results above, provide a comprehensive answer about what you see on the screen. Be enthusiastic and reference specific details from the analysis."})
                            
                            final_response = await model_client.generate(
                                messages=messages,
                                temperature=0.7,
                                max_tokens=512
                            )
                            
                            if final_response and 'content' in final_response:
                                return final_response['content']
                    
                    # If no tool calls or tool execution failed, fall back to direct response
                    elif response_data and 'content' in response_data:
                        return response_data['content']
                        
                except Exception as e:
                    self.logger.error(f"[VisionAwareConversation] ❌ Tool-based approach failed: {e}")
            
            # Fallback: Manual screenshot capture and analysis
            self.logger.info("[VisionAwareConversation] 🔄 Falling back to manual screenshot capture")
            
            try:
                # Manually capture and analyze screenshot
                if hasattr(self.app_context, 'vision_tools') and self.app_context.vision_tools:
                    screenshot_result = await self.app_context.vision_tools.capture_screenshot()
                    if screenshot_result and screenshot_result.success:
                        self.logger.info("[VisionAwareConversation] ✅ Screenshot captured successfully")
                        
                        analysis_result = await self.app_context.vision_tools.analyze_screenshot()
                        if analysis_result and analysis_result.success:
                            self.logger.info("[VisionAwareConversation] ✅ Screenshot analyzed successfully")
                            
                            # Create final message with analysis
                            final_messages = [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": f"User asks: {message}"},
                                {"role": "assistant", "content": f"I've analyzed your screen and here's what I see: {analysis_result.data}"},
                                {"role": "user", "content": "Based on this analysis, provide a helpful and enthusiastic response in your DANZAR personality."}
                            ]
                            
                            final_response = await model_client.generate(
                                messages=final_messages,
                                temperature=0.7,
                                max_tokens=512
                            )
                            
                            if final_response and 'content' in final_response:
                                return final_response['content']
            
            except Exception as e:
                self.logger.error(f"[VisionAwareConversation] ❌ Manual screenshot approach failed: {e}")
            
            # Final fallback: Generic response
            return "I'm having trouble accessing my vision tools right now, but I'm here to help with anything else!"
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] ❌ Screenshot query handling failed: {e}")
            return "I encountered an error while trying to see your screen. Let me know if you need help with anything else!"
    
    async def _generate_enhanced_context_response(self, turn: ConversationTurn, conversation_context: Optional[Dict[str, Any]]) -> str:
        """Generate a response using the LangChain agent for tool awareness."""
        try:
            # Try to use LangChain agent if available
            langchain_tools = getattr(self.app_context, 'langchain_tools', None)
            if langchain_tools and hasattr(langchain_tools, 'agent') and langchain_tools.agent:
                self.logger.info("[VisionAwareConversation] Using LangChain agent for tool-aware response")
                
                # Build context for the agent
                context_info = self._build_agent_context(turn, conversation_context)
                
                # Use the LangChain agent
                response = await langchain_tools.agent.ainvoke({
                    "input": f"{context_info}\n\nUser: {turn.user_input}"
                })
                
                return response.get('output', 'I had trouble processing that request.')
            
            # Fallback to direct LLM if LangChain not available
            self.logger.warning("[VisionAwareConversation] LangChain agent not available, using direct LLM")
            return await self._generate_direct_llm_response(turn)
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Response generation error: {e}")
            return "I'm having trouble thinking about that right now."
    
    def _build_agent_context(self, turn: ConversationTurn, conversation_context: Optional[Dict[str, Any]]) -> str:
        """Build context information for the LangChain agent."""
        context_parts = []
        
        # Visual context
        if turn.visual_context:
            visual = turn.visual_context
            context_parts.append(f"Current visual context: {visual.scene_summary}")
            
            if visual.detected_objects:
                objects = [f"{obj.get('label', 'unknown')}" for obj in visual.detected_objects[:3]]
                context_parts.append(f"Detected objects: {', '.join(objects)}")
            
            if visual.ocr_text:
                texts = [f'"{text}"' for text in visual.ocr_text[:3] if text.strip()]
                if texts:
                    context_parts.append(f"Screen text: {', '.join(texts)}")
        
        # Conversation context
        if conversation_context:
            stm_entries = conversation_context.get('stm_entries', [])
            if stm_entries:
                recent_context = []
                for entry in stm_entries[-3:]:  # Last 3 entries
                    if entry.entry_type == 'user_input':
                        recent_context.append(f"User: {entry.content}")
                    elif entry.entry_type == 'bot_response':
                        recent_context.append(f"You: {entry.content}")
                
                if recent_context:
                    context_parts.append("Recent conversation:")
                    context_parts.extend(recent_context)
        
        # Game context
        game_context = self._get_game_context()
        if game_context:
            context_parts.append(f"Game: {game_context.get('game_name', 'Unknown')}")
        
        return "\n".join(context_parts) if context_parts else "No additional context available."
    
    async def _generate_direct_llm_response(self, turn: ConversationTurn) -> str:
        """Generate a direct LLM response using profile-based system prompts."""
        try:
            # Get profile-based system prompt
            system_prompt = "You are DANZAR, a vision-capable gaming assistant with a witty personality."
            if self.app_context and hasattr(self.app_context, 'active_profile') and self.app_context.active_profile:
                system_prompt = self.app_context.active_profile.system_prompt_chat
            
            # Build enhanced prompt with visual context
            enhanced_prompt = self._build_enhanced_visual_aware_prompt(turn)
            
            # Create proper message structure with system role
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Get model client directly from app context
            model_client = getattr(self.app_context, 'model_client', None)
            if not model_client:
                # Try alternative attribute names
                model_client = getattr(self.app_context, 'llm_client', None)
            if not model_client:
                # Try getting from services
                model_client = self.app_context.get_service('model_client')
            if not model_client:
                self.logger.error("[VisionAwareConversation] ❌ No model client available")
                return "I'm having trouble connecting to my language model right now."
            
            # Generate response using proper message structure
            response_data = await model_client.generate(
                messages=messages,
                temperature=0.7,
                max_tokens=512
            )
            
            if response_data and 'content' in response_data:
                return response_data['content']
            else:
                self.logger.error("[VisionAwareConversation] ❌ No content in LLM response")
                return "I'm having trouble generating a response right now."
                
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] ❌ Direct LLM response generation error: {e}")
            return "I'm having trouble thinking about that right now."
    
    def _update_conversation_summary(self, turn: ConversationTurn):
        """Update conversation summary to maintain short-term memory."""
        try:
            # Get recent conversation turns (last 5 turns)
            recent_turns = list(self.conversation_history)[-5:]
            
            # Create a summary of recent conversation
            summary_parts = []
            for t in recent_turns:
                if t.user_input and t.response:
                    summary_parts.append(f"User: {t.user_input[:100]}...")
                    summary_parts.append(f"Danzar: {t.response[:100]}...")
            
            self.conversation_summary = "\n".join(summary_parts)
            
            # Clean up old conversation history (older than 5 minutes)
            current_time = time.time()
            self.conversation_history = deque([
                t for t in self.conversation_history 
                if current_time - t.timestamp < self.conversation_context_window
            ], maxlen=50)
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Error updating conversation summary: {e}")
    
    async def _generate_visual_aware_response(self, turn: ConversationTurn) -> str:
        """Generate a response that naturally incorporates visual context and conversation memory."""
        try:
            # Get profile-based system prompt
            system_prompt = "You are DANZAR, a vision-capable gaming assistant with a witty personality."
            if self.app_context and hasattr(self.app_context, 'active_profile') and self.app_context.active_profile:
                system_prompt = self.app_context.active_profile.system_prompt_chat
            
            # Build enhanced prompt with visual context
            enhanced_prompt = self._build_enhanced_visual_aware_prompt(turn)
            
            # Create proper message structure with system role
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Get model client directly from app context
            model_client = getattr(self.app_context, 'model_client', None)
            if not model_client:
                # Try alternative attribute names
                model_client = getattr(self.app_context, 'llm_client', None)
            if not model_client:
                # Try getting from services
                model_client = self.app_context.get_service('model_client')
            if not model_client:
                self.logger.error("[VisionAwareConversation] ❌ No model client available")
                return "I'm not connected to my language model right now."
            
            # Generate response using proper message structure
            response_data = await model_client.generate(
                messages=messages,
                temperature=0.7,
                max_tokens=512
            )
            
            if response_data and 'content' in response_data:
                return response_data['content']
            else:
                return "I'm having trouble thinking about that right now."
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Response generation error: {e}")
            return "I'm having trouble thinking about that right now."
    
    def _build_enhanced_visual_aware_prompt(self, turn: ConversationTurn) -> str:
        """Build an enhanced prompt that includes visual context and CLIP insights."""
        prompt_parts = []
        
        # Visual context
        if turn.visual_context:
            visual = turn.visual_context
            prompt_parts.append(f"CURRENT VISUAL CONTEXT:")
            prompt_parts.append(f"- Scene: {visual.scene_summary}")
            prompt_parts.append(f"- Confidence: {visual.confidence:.2f}")
            
            # Add CLIP insights if available
            if visual.clip_insights and visual.clip_insights.get('clip_insights'):
                clip_insights = visual.clip_insights['clip_insights']
                if clip_insights:
                    prompt_parts.append(f"- Visual Understanding: {', '.join([insight['description'] for insight in clip_insights[:3]])}")
            
            # Add detected objects
            if visual.detected_objects:
                objects = [f"{obj.get('label', 'unknown')} ({obj.get('confidence', 0):.2f})" 
                          for obj in visual.detected_objects[:3]]
                prompt_parts.append(f"- Detected Objects: {', '.join(objects)}")
            
            # Add OCR text
            if visual.ocr_text:
                texts = [f'"{text}"' for text in visual.ocr_text[:3] if text.strip()]
                if texts:
                    prompt_parts.append(f"- Screen Text: {', '.join(texts)}")
        
        # Conversation context
        if self.conversation_summary:
            prompt_parts.append(f"CONVERSATION CONTEXT:")
            prompt_parts.append(self.conversation_summary)
        
        # Recent conversation history
        recent_turns = list(self.conversation_history)[-3:]  # Last 3 turns
        if recent_turns:
            prompt_parts.append(f"RECENT CONVERSATION:")
            for i, recent_turn in enumerate(recent_turns):
                if recent_turn != turn:  # Don't include current turn
                    prompt_parts.append(f"- User: {recent_turn.user_input}")
                    if recent_turn.response:
                        prompt_parts.append(f"- You: {recent_turn.response}")
        
        # Game context
        game_context = self._get_game_context()
        if game_context:
            prompt_parts.append(f"GAME CONTEXT:")
            prompt_parts.append(f"- Game: {game_context.get('game_name', 'Unknown')}")
            prompt_parts.append(f"- Context: {game_context.get('context', 'General gameplay')}")
        
        # User message
        prompt_parts.append(f"USER MESSAGE: {turn.user_input}")
        
        # Instructions
        prompt_parts.append(f"INSTRUCTIONS:")
        prompt_parts.append("- Respond naturally and conversationally")
        prompt_parts.append("- Reference what you can see in the game when relevant")
        prompt_parts.append("- Provide helpful gaming advice and commentary")
        prompt_parts.append("- Keep responses concise but informative")
        prompt_parts.append("- If you can't see anything relevant, ask for clarification")
        
        return "\n".join(prompt_parts)
    
    async def query_visual_concept(self, query: str) -> Dict[str, Any]:
        """
        Query CLIP about a specific visual concept in the current frame.
        
        Args:
            query: Natural language query about what to look for
            
        Returns:
            CLIP's response about the query
        """
        if not self.clip_enhancer:
            return {'confidence': 0.0, 'description': 'CLIP not available'}
        
        try:
            # Get current frame from vision data
            vision_data = getattr(self.app_context, 'latest_vision_data', None)
            if not vision_data or not vision_data.get('current_frame'):
                return {'confidence': 0.0, 'description': 'No current frame available'}
            
            current_frame = vision_data['current_frame']
            
            # Query CLIP
            result = self.clip_enhancer.query_visual_concept(current_frame, query)
            return result
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Error querying visual concept: {e}")
            return {'confidence': 0.0, 'description': f'Error querying: {str(e)}'}
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """Get memory statistics including enhanced memory system."""
        stats = {
            'enhanced_memory_available': self.conversation_memory is not None,
            'clip_enabled': self.clip_enabled,
            'vision_integration_enabled': self.vision_integration_enabled,
            'current_visual_context': self.current_visual_context is not None,
            'conversation_history_length': len(self.conversation_history),
            'visual_context_history_length': len(self.visual_context_history)
        }
        
        # Add enhanced memory stats if available
        if self.conversation_memory:
            try:
                memory_stats = self.conversation_memory.get_memory_stats()
                stats['enhanced_memory_stats'] = memory_stats
            except Exception as e:
                stats['enhanced_memory_error'] = str(e)
        
        return stats
    
    def cleanup(self):
        """Cleanup resources."""
        try:
            if self.conversation_memory:
                self.conversation_memory.stop_background_tasks()
                self.logger.info("[VisionAwareConversation] Enhanced memory cleanup completed")
            
            if self.clip_enhancer:
                # Add any CLIP cleanup if needed
                pass
            
            self.logger.info("[VisionAwareConversation] Cleanup completed")
            
        except Exception as e:
            self.logger.error(f"[VisionAwareConversation] Cleanup error: {e}") 